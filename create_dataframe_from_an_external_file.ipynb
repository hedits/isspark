{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criar PySpark DataFrame a partir de um arquivo externo\n",
    "- usar método *read()* para importar arquivos externos\n",
    "- Retornará um Spark DataFrame\n",
    "- Formato de arquivos externos que podem ser importados inclui JSON, TXT ou CSV. \n",
    "\n",
    "\n",
    "&nbsp;\n",
    "_Para trabalhar com arquivos externos requer apenas a sessão do Spark._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('PySpark DataFrame de arquivos externos').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo um arquivo CSV\n",
    "- *spark.read.csv()*, passando o nome do arquivo CSV\n",
    "- *sep* delimitador (virgula) usado no arquivo\n",
    "- *inferSchema* como True, passa pelo aruivo CSV e adaptará automaticamente o esquema no PySpark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv\n",
    "csv_file = spark.read.csv('files/fish.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo um arquivo TXT \n",
    "- *spark.read.text()* recebe o nome/path do arquivo\n",
    "- Cada linha neste arquivo de texto atuará como uma nova linha\n",
    "- Útil quando queremos ler varias linhas de um vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Em outra pasta\n",
    "txt_file = spark.read.text(\"files/example.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo um arquivo JSON\n",
    "- spark.read.json() passamos o nome do arquivo como argumentos\n",
    "- Define o atributo *multiLine=True* para dados de varias linhas\n",
    "- *inferSchema* por padrão é definido como True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo um arquivo JSON\n",
    "json_file = spark.read.json(\"files/sample.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificando tipos de dados dos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(csv_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(txt_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(json_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificando schema do DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Species: string (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Length1: double (nullable = true)\n",
      " |-- Length2: double (nullable = true)\n",
      " |-- Length3: double (nullable = true)\n",
      " |-- Height: double (nullable = true)\n",
      " |-- Width: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt_file.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employees: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- firstName: string (nullable = true)\n",
      " |    |    |-- lastName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_file.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lendo vários arquivos de uma vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazena  o nome/path dos arquivos na variável file\n",
    "files = ['files/fish.csv', 'files/salary.csv']\n",
    "# file é passada como argumento de read.csv\n",
    "df = spark.read.csv(files, sep = ',' , inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+-------+-------+------+\n",
      "|Species|Weight|Length1|Length2|Length3| Height| Width|\n",
      "+-------+------+-------+-------+-------+-------+------+\n",
      "|  Bream| 242.0|   23.2|   25.4|   30.0|  11.52|  4.02|\n",
      "|  Bream| 290.0|   24.0|   26.3|   31.2|  12.48|4.3056|\n",
      "|  Bream| 340.0|   23.9|   26.5|   31.1|12.3778|4.6961|\n",
      "|  Bream| 363.0|   26.3|   29.0|   33.5|  12.73|4.4555|\n",
      "|  Bream| 430.0|   26.5|   29.0|   34.0| 12.444| 5.134|\n",
      "+-------+------+-------+-------+-------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show DataFrame\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7362c36e223275ae100eae7302e8620409b8912dde4e95198ad2c89f03ded0c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
